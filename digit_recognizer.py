"""digit_recognizer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gzjtp6RJe6DQ6Xe2ooN62jf7HRR3eSWv
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
training_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/digit-recognizer/train.csv')
test_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/digit-recognizer/test.csv')
y = training_data['label']
x = training_data.drop(labels=['label'], axis=1)

x = x.values.reshape(-1,28,28,1) #since we need 4D shape for cnn
test_data = test_data.values.reshape(-1,28,28,1)

#print(x_train.shape)

"""
plt.figure()
plt.imshow(x_train[2])
plt.colorbar()
plt.grid(False)
"""

x = x/255.0
test_data = test_data/255.0

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)

#print(x_train.shape)

from keras.utils.np_utils import to_categorical # convert to one-hot-encoding
y_train = to_categorical(y_train, num_classes=10)
y_test = to_categorical(y_test,num_classes=10)

from keras.models import Sequential
from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout

classifier = Sequential()

classifier.add(Conv2D(32,kernel_size=(5,5), input_shape=(x_train[0].shape), activation='relu', padding='same'))
classifier.add(MaxPool2D(pool_size=(2,2)))

classifier.add(Conv2D(32,kernel_size=(5,5), activation='relu', padding='same'))
classifier.add(MaxPool2D(pool_size=(2,2)))
classifier.add(Dropout(0.25))

classifier.add(Conv2D(64,kernel_size=(5,5), activation='relu', padding='same'))
classifier.add(Conv2D(64,kernel_size=(5,5), activation='relu', padding='same'))
classifier.add(MaxPool2D(pool_size=(2,2)))
classifier.add(Dropout(0.25))

classifier.add(Flatten())

classifier.add(Dense(256, activation='relu'))
classifier.add(Dropout(0.5))
classifier.add(Dense(10,activation='softmax'))

#setting optimizer
from keras.optimizers import RMSprop

optimizer = RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-08, decay=0.0)

classifier.compile(optimizer=optimizer,loss='categorical_crossentropy', metrics=['accuracy'])

"""Once our layers are added to the model, we need to set up a score function, a loss function and an optimisation algorithm.

We define the loss function to measure how poorly our model performs on images with known labels. It is the error rate between the oberved labels and the predicted ones. We use a specific form for categorical classifications (>2 classes) called the "categorical_crossentropy".

The most important function is the optimizer. This function will iteratively improve parameters (filters kernel values, weights and bias of neurons ...) in order to minimise the loss.

I choosed RMSprop (with default values), it is a very effective optimizer. The RMSProp update adjusts the Adagrad method in a very simple way in an attempt to reduce its aggressive, monotonically decreasing learning rate. We could also have used Stochastic Gradient Descent ('sgd') optimizer, but it is slower than RMSprop.

The metric function "accuracy" is used is to evaluate the performance our model. This metric function is similar to the loss function, except that the results from the metric evaluation are not used when training the model (only for evaluation).
"""

from keras.callbacks import ReduceLROnPlateau
lr_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=0.5, min_lr=0.00001)

"""monitor: quantity to be monitored.

factor: factor by which the learning rate will be reduced. new_lr = lr * factor

patience: number of epochs with no improvement after which learning rate will be reduced.

verbose: int. 0: quiet, 1: update messages.

mode: one of {auto, min, max}. In min mode, lr will be reduced when the quantity monitored has stopped decreasing; in max mode it will be reduced when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity.

min_delta: threshold for measuring the new optimum, to only focus on significant changes.

cooldown: number of epochs to wait before resuming normal operation after lr has been reduced.

min_lr: lower bound on the learning rate.
"""

# With data augmentation to prevent overfitting
from keras.preprocessing.image import ImageDataGenerator
datagen = ImageDataGenerator(
    featurewise_center=False,
    featurewise_std_normalization=False,
    rotation_range=12,
    zoom_range=0.1,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=False)
datagen.fit(x_train)

epochs=  30
batch_size = 86

classifier.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),
                    steps_per_epoch=len(x_train)//batch_size , epochs=epochs, verbose=2, 
                    callbacks=[lr_reduction], validation_data=(x_test,y_test),
                    validation_steps=len(x_test)//batch_size)

results = classifier.predict(test_data)
results = np.argmax(results, axis=1)
results = pd.Series(results, name='Label')

submission = pd.concat([pd.Series(range(1,28001),name = "ImageId"),results],axis = 1)

submission.to_csv("cnn_mnist.csv",index=False)

#accuracy_of_test_data = 0.989